{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# requires_grad를 True로 설정하면 해당 텐서의 연산 내역을 추적함\n",
    "# 이후 .backward()를 호출하여 모든 변화량을 계산할 수 있으며, 결과는 .grad에 누적됨\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[1., 1.],\n        [1., 1.]])\ntensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\n"
    }
   ],
   "source": [
    "# detach()를 호출하여 해당 텐서의 연산 추적을 멈춤\n",
    "xd = x.detach()\n",
    "print(xd)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.no_grad()를 사용하여 requires_grad=True인 텐서의 연산 추적을 멈춤\n",
    "with torch.no_grad():\n",
    "    # 텐서 연산.. (추적 X)\n",
    "    pass\n",
    "\n",
    "# 텐서 연산.. (추적 O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<AddBackward0 object at 0x11fea89b0>\nNone\n"
    }
   ],
   "source": [
    "# 모든 텐서는 Function과 연결되어있음\n",
    "# (.grad_fn 속성에 연결되어있음)\n",
    "\n",
    "# 사용자가 생성한 텐서의 .grad_fn은 기본값이 None이다.\n",
    "# 텐서끼리 연산하여 나온 결과 텐서의 .grad_fn에는 해당 연산과 관련된 Function이 연결된다\n",
    "print((x + 1).grad_fn)\n",
    "print((xd + 1).grad_fn) # 연산을 추적하지 않기 때문에 기본값인 None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[27., 27.],\n        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<PowBackward0 at 0x11fea8f28>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.ones(2, 4)\n",
    "t = t * 2 # 추적 X\n",
    "t.requires_grad_(True) # 추적하도록 변경 (기존 텐서 덮어쓰기)\n",
    "(t ** 2).grad_fn # 추적 O"
   ]
  }
 ]
}